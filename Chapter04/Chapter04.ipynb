{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=100, output_dim=1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(7*7*128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Reshape((7, 7, 128), input_shape=(7*7*128,)))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Convolution2D(64, 5, 5, border_mode='same', dim_ordering='tf'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Convolution2D(1, 5, 5, border_mode='same', dim_ordering='tf'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(\n",
    "                        64, 5, 5,\n",
    "                        border_mode='same',\n",
    "                        input_shape=(28, 28, 1),\n",
    "                        dim_ordering='tf'\n",
    "    ))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(128, 5, 5, dim_ordering='tf'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_containing_discriminator(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[0, :, :]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def train(BATCH_SIZE):\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    img_rows, img_cols = X_train.shape[1], X_train.shape[2]\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:])\n",
    "    discriminator = discriminator_model()\n",
    "    generator = generator_model()\n",
    "    discriminator_on_generator = \\\n",
    "        generator_containing_discriminator(generator, discriminator)\n",
    "    d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    g_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    discriminator_on_generator.compile(\n",
    "        loss='binary_crossentropy', optimizer=g_optim)\n",
    "    discriminator.trainable = True\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    noise = np.zeros((BATCH_SIZE, 100))\n",
    "    for epoch in range(100):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            for i in range(BATCH_SIZE):\n",
    "                noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            image_batch = image_batch.reshape(image_batch.shape[0], \n",
    "                                              image_batch.shape[2], \n",
    "                                              image_batch.shape[3],\n",
    "                                              image_batch.shape[1])\n",
    "            generated_images = generator.predict(noise, verbose=0)\n",
    "            if index % 20 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                Image.fromarray(image.astype(np.uint8)).save(\n",
    "                    str(epoch)+\"_\"+str(index)+\".png\")\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
    "            d_loss = discriminator.train_on_batch(X, y)\n",
    "            print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "            for i in range(BATCH_SIZE):\n",
    "                noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "            discriminator.trainable = False\n",
    "            g_loss = discriminator_on_generator.train_on_batch(\n",
    "                noise, [1] * BATCH_SIZE)\n",
    "            discriminator.trainable = True\n",
    "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            if index % 10 == 9:\n",
    "                generator.save_weights('generator', True)\n",
    "                discriminator.save_weights('discriminator', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), padding=\"same\", input_shape=(28, 28, 1..., data_format=\"channels_last\")`\n",
      "  import sys\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5), data_format=\"channels_last\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=100, units=1024)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), padding=\"same\", data_format=\"channels_last\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/conda/lib/python3.5/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (5, 5), padding=\"same\", data_format=\"channels_last\")`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0\n",
      "Number of batches 600\n",
      "batch 0 d_loss : 0.683326\n",
      "batch 0 g_loss : 0.680328\n",
      "batch 1 d_loss : 0.675343\n",
      "batch 1 g_loss : 0.674145\n",
      "batch 2 d_loss : 0.656332\n",
      "batch 2 g_loss : 0.672466\n",
      "batch 3 d_loss : 0.633329\n",
      "batch 3 g_loss : 0.658737\n",
      "batch 4 d_loss : 0.623384\n",
      "batch 4 g_loss : 0.649555\n",
      "batch 5 d_loss : 0.604494\n",
      "batch 5 g_loss : 0.642582\n",
      "batch 6 d_loss : 0.590999\n",
      "batch 6 g_loss : 0.635607\n",
      "batch 7 d_loss : 0.579194\n",
      "batch 7 g_loss : 0.623267\n",
      "batch 8 d_loss : 0.560703\n",
      "batch 8 g_loss : 0.613716\n",
      "batch 9 d_loss : 0.545639\n",
      "batch 9 g_loss : 0.611879\n",
      "batch 10 d_loss : 0.522992\n",
      "batch 10 g_loss : 0.595596\n",
      "batch 11 d_loss : 0.512627\n",
      "batch 11 g_loss : 0.590097\n",
      "batch 12 d_loss : 0.514507\n",
      "batch 12 g_loss : 0.584211\n",
      "batch 13 d_loss : 0.510281\n",
      "batch 13 g_loss : 0.572500\n",
      "batch 14 d_loss : 0.487477\n",
      "batch 14 g_loss : 0.569034\n",
      "batch 15 d_loss : 0.474586\n",
      "batch 15 g_loss : 0.566855\n",
      "batch 16 d_loss : 0.471429\n",
      "batch 16 g_loss : 0.559553\n",
      "batch 17 d_loss : 0.464412\n",
      "batch 17 g_loss : 0.554326\n",
      "batch 18 d_loss : 0.464924\n",
      "batch 18 g_loss : 0.545878\n",
      "batch 19 d_loss : 0.462933\n",
      "batch 19 g_loss : 0.540203\n",
      "batch 20 d_loss : 0.458329\n",
      "batch 20 g_loss : 0.545245\n",
      "batch 21 d_loss : 0.456065\n",
      "batch 21 g_loss : 0.533766\n",
      "batch 22 d_loss : 0.457779\n",
      "batch 22 g_loss : 0.537944\n",
      "batch 23 d_loss : 0.451227\n",
      "batch 23 g_loss : 0.534848\n",
      "batch 24 d_loss : 0.448199\n",
      "batch 24 g_loss : 0.533185\n",
      "batch 25 d_loss : 0.452018\n",
      "batch 25 g_loss : 0.532714\n",
      "batch 26 d_loss : 0.449982\n",
      "batch 26 g_loss : 0.531183\n",
      "batch 27 d_loss : 0.462640\n",
      "batch 27 g_loss : 0.524006\n",
      "batch 28 d_loss : 0.463053\n",
      "batch 28 g_loss : 0.527528\n",
      "batch 29 d_loss : 0.454992\n",
      "batch 29 g_loss : 0.531712\n",
      "batch 30 d_loss : 0.455599\n",
      "batch 30 g_loss : 0.531373\n",
      "batch 31 d_loss : 0.459685\n",
      "batch 31 g_loss : 0.527064\n",
      "batch 32 d_loss : 0.462814\n",
      "batch 32 g_loss : 0.534219\n",
      "batch 33 d_loss : 0.465745\n",
      "batch 33 g_loss : 0.537856\n",
      "batch 34 d_loss : 0.462780\n",
      "batch 34 g_loss : 0.539175\n",
      "batch 35 d_loss : 0.470550\n",
      "batch 35 g_loss : 0.540810\n",
      "batch 36 d_loss : 0.481499\n",
      "batch 36 g_loss : 0.544227\n",
      "batch 37 d_loss : 0.479864\n",
      "batch 37 g_loss : 0.544825\n",
      "batch 38 d_loss : 0.475341\n",
      "batch 38 g_loss : 0.549306\n",
      "batch 39 d_loss : 0.477535\n",
      "batch 39 g_loss : 0.558027\n",
      "batch 40 d_loss : 0.482601\n",
      "batch 40 g_loss : 0.550824\n",
      "batch 41 d_loss : 0.479546\n",
      "batch 41 g_loss : 0.562883\n",
      "batch 42 d_loss : 0.489352\n",
      "batch 42 g_loss : 0.563750\n",
      "batch 43 d_loss : 0.485672\n",
      "batch 43 g_loss : 0.569378\n",
      "batch 44 d_loss : 0.486202\n",
      "batch 44 g_loss : 0.580604\n",
      "batch 45 d_loss : 0.498451\n",
      "batch 45 g_loss : 0.586844\n",
      "batch 46 d_loss : 0.505169\n",
      "batch 46 g_loss : 0.588207\n",
      "batch 47 d_loss : 0.508178\n",
      "batch 47 g_loss : 0.597732\n",
      "batch 48 d_loss : 0.506916\n",
      "batch 48 g_loss : 0.596379\n",
      "batch 49 d_loss : 0.493349\n",
      "batch 49 g_loss : 0.619271\n",
      "batch 50 d_loss : 0.496184\n",
      "batch 50 g_loss : 0.616884\n",
      "batch 51 d_loss : 0.501078\n",
      "batch 51 g_loss : 0.622523\n",
      "batch 52 d_loss : 0.492301\n",
      "batch 52 g_loss : 0.627675\n",
      "batch 53 d_loss : 0.498455\n",
      "batch 53 g_loss : 0.639374\n",
      "batch 54 d_loss : 0.491989\n",
      "batch 54 g_loss : 0.647986\n",
      "batch 55 d_loss : 0.487507\n",
      "batch 55 g_loss : 0.649662\n",
      "batch 56 d_loss : 0.493390\n",
      "batch 56 g_loss : 0.670267\n",
      "batch 57 d_loss : 0.479084\n",
      "batch 57 g_loss : 0.668385\n",
      "batch 58 d_loss : 0.477602\n",
      "batch 58 g_loss : 0.683682\n",
      "batch 59 d_loss : 0.476653\n",
      "batch 59 g_loss : 0.692113\n",
      "batch 60 d_loss : 0.485962\n",
      "batch 60 g_loss : 0.700809\n",
      "batch 61 d_loss : 0.479835\n",
      "batch 61 g_loss : 0.708086\n",
      "batch 62 d_loss : 0.459145\n",
      "batch 62 g_loss : 0.722334\n",
      "batch 63 d_loss : 0.461175\n",
      "batch 63 g_loss : 0.724737\n",
      "batch 64 d_loss : 0.462549\n",
      "batch 64 g_loss : 0.738766\n",
      "batch 65 d_loss : 0.454692\n",
      "batch 65 g_loss : 0.750783\n",
      "batch 66 d_loss : 0.454013\n",
      "batch 66 g_loss : 0.761711\n",
      "batch 67 d_loss : 0.452488\n",
      "batch 67 g_loss : 0.771632\n",
      "batch 68 d_loss : 0.447334\n",
      "batch 68 g_loss : 0.792357\n",
      "batch 69 d_loss : 0.436355\n",
      "batch 69 g_loss : 0.778527\n",
      "batch 70 d_loss : 0.440693\n",
      "batch 70 g_loss : 0.802633\n",
      "batch 71 d_loss : 0.428517\n",
      "batch 71 g_loss : 0.804109\n",
      "batch 72 d_loss : 0.431650\n",
      "batch 72 g_loss : 0.821445\n",
      "batch 73 d_loss : 0.432239\n",
      "batch 73 g_loss : 0.817174\n",
      "batch 74 d_loss : 0.423604\n",
      "batch 74 g_loss : 0.823478\n",
      "batch 75 d_loss : 0.417556\n",
      "batch 75 g_loss : 0.838440\n",
      "batch 76 d_loss : 0.411056\n",
      "batch 76 g_loss : 0.837153\n",
      "batch 77 d_loss : 0.394816\n",
      "batch 77 g_loss : 0.849493\n",
      "batch 78 d_loss : 0.397351\n",
      "batch 78 g_loss : 0.849732\n",
      "batch 79 d_loss : 0.391692\n",
      "batch 79 g_loss : 0.862280\n",
      "batch 80 d_loss : 0.388968\n",
      "batch 80 g_loss : 0.870283\n",
      "batch 81 d_loss : 0.383630\n",
      "batch 81 g_loss : 0.874989\n",
      "batch 82 d_loss : 0.374496\n",
      "batch 82 g_loss : 0.894197\n",
      "batch 83 d_loss : 0.356622\n",
      "batch 83 g_loss : 0.892804\n",
      "batch 84 d_loss : 0.343137\n",
      "batch 84 g_loss : 0.904476\n",
      "batch 85 d_loss : 0.338009\n",
      "batch 85 g_loss : 0.911137\n",
      "batch 86 d_loss : 0.332522\n",
      "batch 86 g_loss : 0.925295\n",
      "batch 87 d_loss : 0.335303\n",
      "batch 87 g_loss : 0.933581\n",
      "batch 88 d_loss : 0.333819\n",
      "batch 88 g_loss : 0.929216\n",
      "batch 89 d_loss : 0.345170\n",
      "batch 89 g_loss : 0.938084\n",
      "batch 90 d_loss : 0.330940\n",
      "batch 90 g_loss : 0.960374\n",
      "batch 91 d_loss : 0.340159\n",
      "batch 91 g_loss : 0.947294\n",
      "batch 92 d_loss : 0.338736\n",
      "batch 92 g_loss : 0.940667\n",
      "batch 93 d_loss : 0.314327\n",
      "batch 93 g_loss : 0.961522\n",
      "batch 94 d_loss : 0.328147\n",
      "batch 94 g_loss : 0.963151\n",
      "batch 95 d_loss : 0.306005\n",
      "batch 95 g_loss : 0.953043\n",
      "batch 96 d_loss : 0.300870\n",
      "batch 96 g_loss : 0.955043\n",
      "batch 97 d_loss : 0.281454\n",
      "batch 97 g_loss : 0.973552\n",
      "batch 98 d_loss : 0.288321\n",
      "batch 98 g_loss : 0.963148\n",
      "batch 99 d_loss : 0.288859\n",
      "batch 99 g_loss : 0.945284\n",
      "batch 100 d_loss : 0.270496\n",
      "batch 100 g_loss : 0.969474\n",
      "batch 101 d_loss : 0.280251\n",
      "batch 101 g_loss : 0.942637\n",
      "batch 102 d_loss : 0.233127\n",
      "batch 102 g_loss : 0.952394\n",
      "batch 103 d_loss : 0.265679\n",
      "batch 103 g_loss : 0.960265\n",
      "batch 104 d_loss : 0.246126\n",
      "batch 104 g_loss : 0.952800\n",
      "batch 105 d_loss : 0.243112\n",
      "batch 105 g_loss : 0.971635\n",
      "batch 106 d_loss : 0.265910\n",
      "batch 106 g_loss : 0.943317\n",
      "batch 107 d_loss : 0.253455\n",
      "batch 107 g_loss : 0.984616\n",
      "batch 108 d_loss : 0.229883\n",
      "batch 108 g_loss : 0.925180\n",
      "batch 109 d_loss : 0.233625\n",
      "batch 109 g_loss : 0.942111\n",
      "batch 110 d_loss : 0.228368\n",
      "batch 110 g_loss : 0.943968\n",
      "batch 111 d_loss : 0.231678\n",
      "batch 111 g_loss : 0.941792\n",
      "batch 112 d_loss : 0.254000\n",
      "batch 112 g_loss : 0.929892\n",
      "batch 113 d_loss : 0.232854\n",
      "batch 113 g_loss : 0.915581\n",
      "batch 114 d_loss : 0.229210\n",
      "batch 114 g_loss : 0.917864\n",
      "batch 115 d_loss : 0.245834\n",
      "batch 115 g_loss : 0.930939\n",
      "batch 116 d_loss : 0.233461\n",
      "batch 116 g_loss : 0.894615\n",
      "batch 117 d_loss : 0.228544\n",
      "batch 117 g_loss : 0.907460\n",
      "batch 118 d_loss : 0.230838\n",
      "batch 118 g_loss : 0.882900\n",
      "batch 119 d_loss : 0.221602\n",
      "batch 119 g_loss : 0.894618\n",
      "batch 120 d_loss : 0.231952\n",
      "batch 120 g_loss : 0.854705\n"
     ]
    }
   ],
   "source": [
    "train(BATCH_SIZE=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
